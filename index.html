<!DOCTYPE html>
<head><meta charset="utf-8" /><meta content="minimum-scale=1, initial-scale=1, width=device-width, shrink-to-fit=no" name="viewport" /><link href="static/css/style.css" rel="stylesheet" type="text/css" /><link href="static/css/custom.css" rel="stylesheet" type="text/css" /><link href="static/img/logo.png" rel="shortcut icon" type="image/png" /><link href="static/img/logo.png" rel="shortcut icon" sizes="192x192" /><link href="static/img/logo.png" rel="apple-touch-icon" /><meta name="apple-mobile-web-app-title" /><meta content="yes" name="apple-mobile-web-app-capable" /><meta content="yes" name="apple-touch-fullscreen" /><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" /><meta content="yes" name="mobile-web-app-capable" /><meta property="og:title" /><meta content="site" property="og:type" /><meta content="static/img/logo.png" property="og:image" /><meta property="og:description" /><title></title><meta property="og:site_name" /><meta /></head><body><div id="root"></div><script>window.logseq_db="[&quot;~#datascript/DB&quot;,[&quot;^ &quot;,&quot;~:schema&quot;,[&quot;^ &quot;,&quot;~:ast/version&quot;,[&quot;^ &quot;],&quot;~:db/encryption-keys&quot;,[&quot;^ &quot;],&quot;~:file/content&quot;,[&quot;^ &quot;],&quot;~:git/status&quot;,[&quot;^ &quot;],&quot;~:repo/cloned?&quot;,[&quot;^ &quot;],&quot;~:block/alias&quot;,[&quot;^ &quot;,&quot;~:db/valueType&quot;,&quot;~:db.type/ref&quot;,&quot;~:db/cardinality&quot;,&quot;~:db.cardinality/many&quot;],&quot;~:git/error&quot;,[&quot;^ &quot;],&quot;~:block/pre-block?&quot;,[&quot;^ &quot;],&quot;~:git/last-pulled-at&quot;,[&quot;^ &quot;],&quot;~:block/uuid&quot;,[&quot;^ &quot;,&quot;~:db/unique&quot;,&quot;~:db.unique/identity&quot;],&quot;~:repo/url&quot;,[&quot;^ &quot;,&quot;^@&quot;,&quot;^A&quot;],&quot;~:block/priority&quot;,[&quot;^ &quot;],&quot;~:block/properties&quot;,[&quot;^ &quot;],&quot;~:block/journal?&quot;,[&quot;^ &quot;],&quot;~:block/namespace&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;],&quot;~:block/updated-at&quot;,[&quot;^ &quot;],&quot;~:block/repeated?&quot;,[&quot;^ &quot;],&quot;~:db/type&quot;,[&quot;^ &quot;],&quot;~:file/handle&quot;,[&quot;^ &quot;],&quot;~:block/left&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;],&quot;~:block/refs&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;,&quot;^:&quot;,&quot;^;&quot;],&quot;~:block/scheduled&quot;,[&quot;^ &quot;],&quot;~:me/avatar&quot;,[&quot;^ &quot;],&quot;~:db/encrypted?&quot;,[&quot;^ &quot;],&quot;~:block/properties-order&quot;,[&quot;^ &quot;],&quot;~:block/created-at&quot;,[&quot;^ &quot;],&quot;~:block/deadline&quot;,[&quot;^ &quot;],&quot;~:block/body&quot;,[&quot;^ &quot;],&quot;~:me/name&quot;,[&quot;^ &quot;],&quot;~:block/meta&quot;,[&quot;^ &quot;],&quot;~:block/journal-day&quot;,[&quot;^ &quot;],&quot;~:block/format&quot;,[&quot;^ &quot;],&quot;~:block/level&quot;,[&quot;^ &quot;],&quot;~:block/tags&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;,&quot;^:&quot;,&quot;^;&quot;],&quot;~:block/title&quot;,[&quot;^ &quot;],&quot;~:block/content&quot;,[&quot;^ &quot;],&quot;~:recent/pages&quot;,[&quot;^ &quot;],&quot;~:db/ident&quot;,[&quot;^ &quot;,&quot;^@&quot;,&quot;^A&quot;],&quot;~:block/path-refs&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;,&quot;^:&quot;,&quot;^;&quot;],&quot;~:block/parent&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;],&quot;~:block/heading-level&quot;,[&quot;^ &quot;],&quot;~:block/type&quot;,[&quot;^ &quot;],&quot;~:me/email&quot;,[&quot;^ &quot;],&quot;~:block/page&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;,&quot;~:db/index&quot;,true],&quot;~:block/name&quot;,[&quot;^ &quot;,&quot;^@&quot;,&quot;^A&quot;],&quot;~:file/path&quot;,[&quot;^ &quot;,&quot;^@&quot;,&quot;^A&quot;],&quot;~:block/file&quot;,[&quot;^ &quot;,&quot;^8&quot;,&quot;^9&quot;],&quot;~:block/marker&quot;,[&quot;^ &quot;],&quot;~:block/original-name&quot;,[&quot;^ &quot;,&quot;^@&quot;,&quot;^A&quot;],&quot;~:schema/version&quot;,[&quot;^ &quot;]],&quot;~:datoms&quot;,[&quot;~#list&quot;,[[&quot;~#datascript/Datom&quot;,[1,&quot;^1&gt;&quot;,&quot;0.0.2&quot;,536870913]],[&quot;^1A&quot;,[3,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[3,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[3,&quot;^19&quot;,&quot;todo&quot;,536870915]],[&quot;^1A&quot;,[3,&quot;^1=&quot;,&quot;TODO&quot;,536871169]],[&quot;^1A&quot;,[3,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[3,&quot;^?&quot;,&quot;~u1e6a5aae-f2dc-4e2f-935c-8b0d64825040&quot;,536871169]],[&quot;^1A&quot;,[4,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[4,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[4,&quot;^19&quot;,&quot;now&quot;,536870915]],[&quot;^1A&quot;,[4,&quot;^1=&quot;,&quot;NOW&quot;,536870915]],[&quot;^1A&quot;,[4,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[4,&quot;^?&quot;,&quot;~ubf287b58-58dc-44b9-86ed-5bc7ae01964f&quot;,536871169]],[&quot;^1A&quot;,[5,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[5,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[5,&quot;^19&quot;,&quot;later&quot;,536870915]],[&quot;^1A&quot;,[5,&quot;^1=&quot;,&quot;LATER&quot;,536870915]],[&quot;^1A&quot;,[5,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[5,&quot;^?&quot;,&quot;~u382c9493-91bd-4c8c-9e9b-6fcc5c6010f7&quot;,536871169]],[&quot;^1A&quot;,[6,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[6,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[6,&quot;^19&quot;,&quot;done&quot;,536870915]],[&quot;^1A&quot;,[6,&quot;^1=&quot;,&quot;DONE&quot;,536870915]],[&quot;^1A&quot;,[6,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[6,&quot;^?&quot;,&quot;~uefccad37-c1f5-4558-b35d-e71052304bf6&quot;,536871169]],[&quot;^1A&quot;,[7,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[7,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[7,&quot;^19&quot;,&quot;doing&quot;,536870915]],[&quot;^1A&quot;,[7,&quot;^1=&quot;,&quot;DOING&quot;,536870915]],[&quot;^1A&quot;,[7,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[7,&quot;^?&quot;,&quot;~u67e21f94-d721-4224-bf43-2fcd46532610&quot;,536871169]],[&quot;^1A&quot;,[8,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[8,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[8,&quot;^19&quot;,&quot;in-progress&quot;,536870915]],[&quot;^1A&quot;,[8,&quot;^1=&quot;,&quot;IN-PROGRESS&quot;,536870915]],[&quot;^1A&quot;,[8,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[8,&quot;^?&quot;,&quot;~u38416561-a865-4708-baeb-f386564f7350&quot;,536871169]],[&quot;^1A&quot;,[9,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[9,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[9,&quot;^19&quot;,&quot;c&quot;,536870915]],[&quot;^1A&quot;,[9,&quot;^1=&quot;,&quot;C&quot;,536870915]],[&quot;^1A&quot;,[9,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[9,&quot;^?&quot;,&quot;~u3f46cb2f-60d1-4ba7-a416-9e649e2adccc&quot;,536871169]],[&quot;^1A&quot;,[10,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[10,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[10,&quot;^19&quot;,&quot;b&quot;,536870915]],[&quot;^1A&quot;,[10,&quot;^1=&quot;,&quot;B&quot;,536870915]],[&quot;^1A&quot;,[10,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[10,&quot;^?&quot;,&quot;~ud155b23f-7612-40c7-bdbf-c5cae575b22b&quot;,536871169]],[&quot;^1A&quot;,[11,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[11,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[11,&quot;^19&quot;,&quot;waiting&quot;,536870915]],[&quot;^1A&quot;,[11,&quot;^1=&quot;,&quot;WAITING&quot;,536870915]],[&quot;^1A&quot;,[11,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[11,&quot;^?&quot;,&quot;~u46d28954-a10b-4e0e-a874-61bdc479a14c&quot;,536871169]],[&quot;^1A&quot;,[12,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[12,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[12,&quot;^19&quot;,&quot;a&quot;,536870915]],[&quot;^1A&quot;,[12,&quot;^1=&quot;,&quot;A&quot;,536870915]],[&quot;^1A&quot;,[12,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[12,&quot;^?&quot;,&quot;~ue4ad453a-07d9-4326-8895-76e03874d4c1&quot;,536871169]],[&quot;^1A&quot;,[13,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[13,&quot;^E&quot;,false,536870915]],[&quot;^1A&quot;,[13,&quot;^19&quot;,&quot;wait&quot;,536870915]],[&quot;^1A&quot;,[13,&quot;^1=&quot;,&quot;WAIT&quot;,536870915]],[&quot;^1A&quot;,[13,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[13,&quot;^?&quot;,&quot;~u8f24ac60-952a-41fa-8b21-0259f00927d2&quot;,536871169]],[&quot;^1A&quot;,[23,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[23,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[23,&quot;^19&quot;,&quot;abstract&quot;,536870917]],[&quot;^1A&quot;,[23,&quot;^1=&quot;,&quot;Abstract&quot;,536870917]],[&quot;^1A&quot;,[23,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[23,&quot;^?&quot;,&quot;~u6120a3a8-aa56-4761-846e-06517d4fac5a&quot;,536870917]],[&quot;^1A&quot;,[24,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[24,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[24,&quot;^19&quot;,&quot;journalarticle&quot;,536870917]],[&quot;^1A&quot;,[24,&quot;^1=&quot;,&quot;journalArticle&quot;,536870917]],[&quot;^1A&quot;,[24,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[24,&quot;^?&quot;,&quot;~u6120a3a8-d259-445f-ada0-497ccc4bb616&quot;,536870917]],[&quot;^1A&quot;,[25,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[25,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[25,&quot;^19&quot;,&quot;fuchun peng&quot;,536870917]],[&quot;^1A&quot;,[25,&quot;^1=&quot;,&quot;Fuchun Peng&quot;,536870917]],[&quot;^1A&quot;,[25,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[25,&quot;^?&quot;,&quot;~u6120a3a8-a402-4150-9c22-a7e073b0117e&quot;,536870917]],[&quot;^1A&quot;,[26,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[26,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[26,&quot;^19&quot;,&quot;computer science - computation and language&quot;,536870917]],[&quot;^1A&quot;,[26,&quot;^1=&quot;,&quot;Computer Science - Computation and Language&quot;,536870917]],[&quot;^1A&quot;,[26,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[26,&quot;^?&quot;,&quot;~u6120a545-3bae-483d-a24c-e3597fcf0ed5&quot;,536871036]],[&quot;^1A&quot;,[27,&quot;^Q&quot;,1629346910603,536870918]],[&quot;^1A&quot;,[27,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[27,&quot;^19&quot;,&quot;computer science - sound&quot;,536870917]],[&quot;^1A&quot;,[27,&quot;^1=&quot;,&quot;Computer Science - Sound&quot;,536870917]],[&quot;^1A&quot;,[27,&quot;^G&quot;,1629346910603,536870918]],[&quot;^1A&quot;,[27,&quot;^?&quot;,&quot;~u6120a545-7d65-4803-8780-f028ed7db504&quot;,536871036]],[&quot;^1A&quot;,[28,&quot;^Q&quot;,1629529000772,536870918]],[&quot;^1A&quot;,[28,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[28,&quot;^19&quot;,&quot;speech recognition&quot;,536870917]],[&quot;^1A&quot;,[28,&quot;^1=&quot;,&quot;Speech Recognition&quot;,536870917]],[&quot;^1A&quot;,[28,&quot;^G&quot;,1629529000772,536870918]],[&quot;^1A&quot;,[28,&quot;^?&quot;,&quot;~u6120a3a8-e6db-4726-8569-0ebbec6218f1&quot;,536870917]],[&quot;^1A&quot;,[29,&quot;^Q&quot;,1581638400000,536870918]],[&quot;^1A&quot;,[29,&quot;^V&quot;,20200214,536870917]],[&quot;^1A&quot;,[29,&quot;^E&quot;,true,536870917]],[&quot;^1A&quot;,[29,&quot;^19&quot;,&quot;feb 14th, 2020&quot;,536870917]],[&quot;^1A&quot;,[29,&quot;^1=&quot;,&quot;Feb 14th, 2020&quot;,536870917]],[&quot;^1A&quot;,[29,&quot;^G&quot;,1581638400000,536870918]],[&quot;^1A&quot;,[29,&quot;^?&quot;,&quot;~u6120a3a8-8b32-42ea-baee-9e8dd5f59576&quot;,536870917]],[&quot;^1A&quot;,[30,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[30,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[30,&quot;^19&quot;,&quot;abdelrahman mohamed&quot;,536870917]],[&quot;^1A&quot;,[30,&quot;^1=&quot;,&quot;Abdelrahman Mohamed&quot;,536870917]],[&quot;^1A&quot;,[30,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[30,&quot;^?&quot;,&quot;~u6120a3a8-c548-4dfb-881b-ad455936453a&quot;,536870917]],[&quot;^1A&quot;,[31,&quot;^Q&quot;,1603324800000,536870918]],[&quot;^1A&quot;,[31,&quot;^V&quot;,20201022,536870917]],[&quot;^1A&quot;,[31,&quot;^E&quot;,true,536870917]],[&quot;^1A&quot;,[31,&quot;^19&quot;,&quot;oct 22nd, 2020&quot;,536870917]],[&quot;^1A&quot;,[31,&quot;^1=&quot;,&quot;Oct 22nd, 2020&quot;,536870917]],[&quot;^1A&quot;,[31,&quot;^G&quot;,1603324800000,536870918]],[&quot;^1A&quot;,[31,&quot;^?&quot;,&quot;~u6120a3a8-6ff5-428f-b98c-82df259636c3&quot;,536870917]],[&quot;^1A&quot;,[32,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[32,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[32,&quot;^19&quot;,&quot;ross girshick&quot;,536870917]],[&quot;^1A&quot;,[32,&quot;^1=&quot;,&quot;Ross Girshick&quot;,536870917]],[&quot;^1A&quot;,[32,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[32,&quot;^?&quot;,&quot;~u6120a3a8-0cf6-4e7b-b988-f13d8ddfe646&quot;,536870917]],[&quot;^1A&quot;,[33,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[33,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[33,&quot;^19&quot;,&quot;yatharth saraf&quot;,536870917]],[&quot;^1A&quot;,[33,&quot;^1=&quot;,&quot;Yatharth Saraf&quot;,536870917]],[&quot;^1A&quot;,[33,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[33,&quot;^?&quot;,&quot;~u6120a3a8-c782-42ca-9235-f1d3b9624059&quot;,536870917]],[&quot;^1A&quot;,[34,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[34,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[34,&quot;^19&quot;,&quot;geoffrey zweig&quot;,536870917]],[&quot;^1A&quot;,[34,&quot;^1=&quot;,&quot;Geoffrey Zweig&quot;,536870917]],[&quot;^1A&quot;,[34,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[34,&quot;^?&quot;,&quot;~u6120a3a8-fe2d-4f98-8752-a329b71ffc95&quot;,536870917]],[&quot;^1A&quot;,[35,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[35,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[35,&quot;^19&quot;,&quot;yongqiang wang&quot;,536870917]],[&quot;^1A&quot;,[35,&quot;^1=&quot;,&quot;Yongqiang Wang&quot;,536870917]],[&quot;^1A&quot;,[35,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[35,&quot;^?&quot;,&quot;~u6120a3a8-74d3-4996-aa67-e762d125d1b9&quot;,536870917]],[&quot;^1A&quot;,[37,&quot;^Q&quot;,1629346910602,536870918]],[&quot;^1A&quot;,[37,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[37,&quot;^19&quot;,&quot;michael auli&quot;,536870917]],[&quot;^1A&quot;,[37,&quot;^1=&quot;,&quot;Michael Auli&quot;,536870917]],[&quot;^1A&quot;,[37,&quot;^G&quot;,1629346910602,536870918]],[&quot;^1A&quot;,[37,&quot;^?&quot;,&quot;~u6120a3a8-ce3a-4dad-9230-1e5ede16456f&quot;,536870917]],[&quot;^1A&quot;,[38,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[38,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[38,&quot;^19&quot;,&quot;frank zhang&quot;,536870917]],[&quot;^1A&quot;,[38,&quot;^1=&quot;,&quot;Frank Zhang&quot;,536870917]],[&quot;^1A&quot;,[38,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[38,&quot;^?&quot;,&quot;~u6120a3a8-9911-4a2e-a4d3-767f9938063e&quot;,536870917]],[&quot;^1A&quot;,[40,&quot;^Q&quot;,1629343667322,536871096]],[&quot;^1A&quot;,[40,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[40,&quot;^19&quot;,&quot;contents&quot;,536870917]],[&quot;^1A&quot;,[40,&quot;^1=&quot;,&quot;Contents&quot;,536870917]],[&quot;^1A&quot;,[40,&quot;~:block/unordered&quot;,true,536871074]],[&quot;^1A&quot;,[40,&quot;^G&quot;,1629529953661,536871162]],[&quot;^1A&quot;,[40,&quot;^?&quot;,&quot;~u6120a4ae-af9b-414a-af25-efcb09000f6f&quot;,536870983]],[&quot;^1A&quot;,[41,&quot;^Q&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[41,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[41,&quot;^19&quot;,&quot;dmytro okhonko&quot;,536870917]],[&quot;^1A&quot;,[41,&quot;^1=&quot;,&quot;Dmytro Okhonko&quot;,536870917]],[&quot;^1A&quot;,[41,&quot;^G&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[41,&quot;^?&quot;,&quot;~u6120a3a8-6089-4a46-a252-36d571916b1f&quot;,536870917]],[&quot;^1A&quot;,[42,&quot;^Q&quot;,1629346910603,536870918]],[&quot;^1A&quot;,[42,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[42,&quot;^19&quot;,&quot;henry zhou&quot;,536870917]],[&quot;^1A&quot;,[42,&quot;^1=&quot;,&quot;Henry Zhou&quot;,536870917]],[&quot;^1A&quot;,[42,&quot;^G&quot;,1629346910603,536870918]],[&quot;^1A&quot;,[42,&quot;^?&quot;,&quot;~u6120a3a8-f392-4d6b-84b4-c6bb4411123f&quot;,536870917]],[&quot;^1A&quot;,[43,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[43,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[43,&quot;^19&quot;,&quot;sergey edunov&quot;,536870917]],[&quot;^1A&quot;,[43,&quot;^1=&quot;,&quot;Sergey Edunov&quot;,536870917]],[&quot;^1A&quot;,[43,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[43,&quot;^?&quot;,&quot;~u6120a3a8-2961-4b36-ac25-f029bb3726a6&quot;,536870917]],[&quot;^1A&quot;,[44,&quot;^Q&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[44,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[44,&quot;^19&quot;,&quot;kritika singh&quot;,536870917]],[&quot;^1A&quot;,[44,&quot;^1=&quot;,&quot;Kritika Singh&quot;,536870917]],[&quot;^1A&quot;,[44,&quot;^G&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[44,&quot;^?&quot;,&quot;~u6120a3a8-c0a7-42c1-8190-d7aadefbbff9&quot;,536870917]],[&quot;^1A&quot;,[45,&quot;^Q&quot;,1629346910602,536870918]],[&quot;^1A&quot;,[45,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[45,&quot;^19&quot;,&quot;alexei baevski&quot;,536870917]],[&quot;^1A&quot;,[45,&quot;^1=&quot;,&quot;Alexei Baevski&quot;,536870917]],[&quot;^1A&quot;,[45,&quot;^G&quot;,1629346910602,536870918]],[&quot;^1A&quot;,[45,&quot;^?&quot;,&quot;~u6120a3a8-2c56-4b87-bd5f-969608b4863f&quot;,536870917]],[&quot;^1A&quot;,[46,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[46,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[46,&quot;^19&quot;,&quot;computer science - machine learning&quot;,536870917]],[&quot;^1A&quot;,[46,&quot;^1=&quot;,&quot;Computer Science - Machine Learning&quot;,536870917]],[&quot;^1A&quot;,[46,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[46,&quot;^?&quot;,&quot;~u6120a545-1747-4495-822d-93be8fcbbf17&quot;,536871036]],[&quot;^1A&quot;,[47,&quot;^Q&quot;,1629356502566,536870918]],[&quot;^1A&quot;,[47,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[47,&quot;^19&quot;,&quot;eess]&quot;,536870917]],[&quot;^1A&quot;,[47,&quot;^1=&quot;,&quot;eess]&quot;,536870917]],[&quot;^1A&quot;,[47,&quot;^G&quot;,1629356502566,536870918]],[&quot;^1A&quot;,[47,&quot;^?&quot;,&quot;~u6120a3a8-7702-4868-ab14-efd4269cd87d&quot;,536870917]],[&quot;^1A&quot;,[49,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[49,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[49,&quot;^19&quot;,&quot;attachments&quot;,536870917]],[&quot;^1A&quot;,[49,&quot;^1=&quot;,&quot;Attachments&quot;,536870917]],[&quot;^1A&quot;,[49,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[49,&quot;^?&quot;,&quot;~u6120a3a8-b440-401b-af16-a567916d6df2&quot;,536870917]],[&quot;^1A&quot;,[51,&quot;^Q&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[51,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[51,&quot;^19&quot;,&quot;arxiv:1910.12367 [cs&quot;,536870917]],[&quot;^1A&quot;,[51,&quot;^1=&quot;,&quot;arXiv:1910.12367 [cs&quot;,536870917]],[&quot;^1A&quot;,[51,&quot;^G&quot;,1629526968199,536870918]],[&quot;^1A&quot;,[51,&quot;^?&quot;,&quot;~u6120a3a8-babf-43da-babc-dac1394072b6&quot;,536870917]],[&quot;^1A&quot;,[52,&quot;^Q&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[52,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[52,&quot;^19&quot;,&quot;electrical engineering and systems science - audio and speech processing&quot;,536870917]],[&quot;^1A&quot;,[52,&quot;^1=&quot;,&quot;Electrical Engineering and Systems Science - Audio and Speech Processing&quot;,536870917]],[&quot;^1A&quot;,[52,&quot;^G&quot;,1629343667322,536870918]],[&quot;^1A&quot;,[52,&quot;^?&quot;,&quot;~u6120a545-394c-4078-8d61-933ccb1b486a&quot;,536871036]],[&quot;^1A&quot;,[53,&quot;^Q&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[53,&quot;^E&quot;,false,536870917]],[&quot;^1A&quot;,[53,&quot;^19&quot;,&quot;jun liu&quot;,536870917]],[&quot;^1A&quot;,[53,&quot;^1=&quot;,&quot;Jun Liu&quot;,536870917]],[&quot;^1A&quot;,[53,&quot;^G&quot;,1629526968198,536870918]],[&quot;^1A&quot;,[53,&quot;^?&quot;,&quot;~u6120a3a8-df32-44d3-9605-bb73979b631b&quot;,536870917]],[&quot;^1A&quot;,[63,&quot;~:block/anchor&quot;,&quot;http-3a--2f--2f-www-2e-cs-2e-toronto-2e-edu-2f--7e-asamir-2f-&quot;,536870917]],[&quot;^1A&quot;,[63,&quot;^S&quot;,[],536870917]],[&quot;^1A&quot;,[63,&quot;~:block/children&quot;,[&quot;~#set&quot;,[]],536870917]],[&quot;^1A&quot;,[63,&quot;^[&quot;,&quot;[http://www.cs.toronto.edu/~asamir/](http://www.cs.toronto.edu/~asamir/)&quot;,536870917]],[&quot;^1A&quot;,[63,&quot;^W&quot;,&quot;~:markdown&quot;,536870917]],[&quot;^1A&quot;,[63,&quot;^K&quot;,30,536870917]],[&quot;^1A&quot;,[63,&quot;^X&quot;,1,536870917]],[&quot;^1A&quot;,[63,&quot;^U&quot;,[&quot;^ &quot;,&quot;~:timestamps&quot;,[],&quot;~:properties&quot;,[],&quot;~:start-pos&quot;,0,&quot;~:end-pos&quot;,74],536870917]],[&quot;^1A&quot;,[63,&quot;^17&quot;,30,536870917]],[&quot;^1A&quot;,[63,&quot;^13&quot;,30,536870917]],[&quot;^1A&quot;,[63,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;~:url&quot;,[&quot;Complex&quot;,[&quot;^ &quot;,&quot;~:protocol&quot;,&quot;http&quot;,&quot;~:link&quot;,&quot;www.cs.toronto.edu/~asamir/&quot;]],&quot;~:label&quot;,[[&quot;Plain&quot;,&quot;http://www.cs.toronto.edu/~asamir/&quot;]],&quot;~:full_text&quot;,&quot;[http://www.cs.toronto.edu/~asamir/](http://www.cs.toronto.edu/~asamir/)&quot;,&quot;~:metadata&quot;,&quot;&quot;]]],536870917]],[&quot;^1A&quot;,[63,&quot;^1B&quot;,true,536870917]],[&quot;^1A&quot;,[63,&quot;^?&quot;,&quot;~u6120a3a8-3306-4362-806f-71f059031903&quot;,536870917]],[&quot;^1A&quot;,[99,&quot;^11&quot;,&quot;^10&quot;,536870919]],[&quot;^1A&quot;,[100,&quot;^2&quot;,&quot;0.0.1&quot;,536870920]],[&quot;^1A&quot;,[100,&quot;^11&quot;,&quot;^2&quot;,536870920]],[&quot;^1A&quot;,[101,&quot;^[&quot;,&quot;title:: attachments:wav2vec%202.0%20A%20Framework%20for%20Self-Supervised%20Learning%20of%20Speech%20Representation.pdf&quot;,536870927]],[&quot;^1A&quot;,[101,&quot;^W&quot;,&quot;^1F&quot;,536870927]],[&quot;^1A&quot;,[101,&quot;^=&quot;,true,536870927]],[&quot;^1A&quot;,[101,&quot;^D&quot;,[&quot;^ &quot;,&quot;~:title&quot;,&quot;attachments:wav2vec%202.0%20A%20Framework%20for%20Self-Supervised%20Learning%20of%20Speech%20Representation.pdf&quot;],536870927]],[&quot;^1A&quot;,[101,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^1Q&quot;]],536870927]],[&quot;^1A&quot;,[101,&quot;^1B&quot;,true,536870927]],[&quot;^1A&quot;,[101,&quot;^?&quot;,&quot;~u6120a3c3-295e-4651-9af5-c974c3c8af18&quot;,536870927]],[&quot;^1A&quot;,[102,&quot;^[&quot;,&quot;&quot;,536870927]],[&quot;^1A&quot;,[102,&quot;^W&quot;,&quot;^1F&quot;,536870927]],[&quot;^1A&quot;,[102,&quot;^K&quot;,101,536870927]],[&quot;^1A&quot;,[102,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^1Q&quot;]],536870927]],[&quot;^1A&quot;,[102,&quot;^1B&quot;,true,536870927]],[&quot;^1A&quot;,[102,&quot;^?&quot;,&quot;~u6120a3c3-709b-49fb-8ee1-2d2dfb0bfd09&quot;,536870927]],[&quot;^1A&quot;,[106,&quot;^V&quot;,20210609,536870959]],[&quot;^1A&quot;,[106,&quot;^E&quot;,true,536870959]],[&quot;^1A&quot;,[106,&quot;^19&quot;,&quot;jun 9th, 2021&quot;,536870959]],[&quot;^1A&quot;,[106,&quot;^1=&quot;,&quot;Jun 9th, 2021&quot;,536870959]],[&quot;^1A&quot;,[106,&quot;^?&quot;,&quot;~u6120a475-5004-4880-b116-d93e8d34d27b&quot;,536870959]],[&quot;^1A&quot;,[107,&quot;^E&quot;,false,536870959]],[&quot;^1A&quot;,[107,&quot;^19&quot;,&quot;peter vieting&quot;,536870959]],[&quot;^1A&quot;,[107,&quot;^1=&quot;,&quot;Peter Vieting&quot;,536870959]],[&quot;^1A&quot;,[107,&quot;^?&quot;,&quot;~u6120a475-a524-4dbf-b313-16e5c1e9403c&quot;,536870959]],[&quot;^1A&quot;,[108,&quot;^E&quot;,false,536870959]],[&quot;^1A&quot;,[108,&quot;^19&quot;,&quot;christoph lüscher&quot;,536870959]],[&quot;^1A&quot;,[108,&quot;^1=&quot;,&quot;Christoph Lüscher&quot;,536870959]],[&quot;^1A&quot;,[108,&quot;^?&quot;,&quot;~u6120a475-193d-4a77-bbe2-577a4499905a&quot;,536870959]],[&quot;^1A&quot;,[109,&quot;^E&quot;,false,536870959]],[&quot;^1A&quot;,[109,&quot;^19&quot;,&quot;wilfried michel&quot;,536870959]],[&quot;^1A&quot;,[109,&quot;^1=&quot;,&quot;Wilfried Michel&quot;,536870959]],[&quot;^1A&quot;,[109,&quot;^?&quot;,&quot;~u6120a475-0f66-4b2e-9cd5-26be4d049446&quot;,536870959]],[&quot;^1A&quot;,[110,&quot;^E&quot;,false,536870959]],[&quot;^1A&quot;,[110,&quot;^19&quot;,&quot;ralf schlüter&quot;,536870959]],[&quot;^1A&quot;,[110,&quot;^1=&quot;,&quot;Ralf Schlüter&quot;,536870959]],[&quot;^1A&quot;,[110,&quot;^?&quot;,&quot;~u6120a475-778d-40f7-a49d-5b2b9d6acc88&quot;,536870959]],[&quot;^1A&quot;,[111,&quot;^E&quot;,false,536870959]],[&quot;^1A&quot;,[111,&quot;^19&quot;,&quot;hermann ney&quot;,536870959]],[&quot;^1A&quot;,[111,&quot;^1=&quot;,&quot;Hermann Ney&quot;,536870959]],[&quot;^1A&quot;,[111,&quot;^?&quot;,&quot;~u6120a475-895a-43d1-89f5-2576514de9de&quot;,536870959]],[&quot;^1A&quot;,[115,&quot;^E&quot;,false,536870981]],[&quot;^1A&quot;,[115,&quot;^19&quot;,&quot;feature replacement and combination for hybrid asr systems&quot;,536870981]],[&quot;^1A&quot;,[115,&quot;^1=&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;,536870981]],[&quot;^1A&quot;,[115,&quot;^D&quot;,[&quot;^ &quot;,&quot;~:tags&quot;,[&quot;^1@&quot;,[&quot;Computer Science - Computation and Language&quot;,&quot;Computer Science - Machine Learning&quot;,&quot;Computer Science - Sound&quot;,&quot;Electrical Engineering and Systems Science - Audio and Speech Processing&quot;]],&quot;~:date&quot;,[&quot;^1E&quot;,[&quot;Jun 9th, 2021&quot;]],&quot;~:extra&quot;,&quot;arXiv: 2104.04298&quot;,&quot;^1Q&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;,&quot;~:item-type&quot;,[&quot;^1E&quot;,[&quot;journalArticle&quot;]],&quot;~:access-date&quot;,&quot;2021-07-07T05:39:04Z&quot;,&quot;~:original-title&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;,&quot;^1K&quot;,&quot;http://arxiv.org/abs/2104.04298&quot;,&quot;~:publication-title&quot;,[&quot;^1E&quot;,[&quot;arXiv:2104.04298 [cs&quot;,&quot;eess]&quot;]],&quot;~:authors&quot;,[&quot;^1E&quot;,[&quot;Peter Vieting&quot;,&quot;Christoph Lüscher&quot;,&quot;Wilfried Michel&quot;,&quot;Ralf Schlüter&quot;,&quot;Hermann Ney&quot;]],&quot;~:library-catalog&quot;,&quot;arXiv.org&quot;,&quot;~:links&quot;,&quot;[Local library](zotero://select/library/items/5JFDDHFZ), [Web library](https://www.zotero.org/users/7048753/items/5JFDDHFZ)&quot;],536870981]],[&quot;^1A&quot;,[115,&quot;^Y&quot;,26,536870981]],[&quot;^1A&quot;,[115,&quot;^Y&quot;,27,536870981]],[&quot;^1A&quot;,[115,&quot;^Y&quot;,46,536870981]],[&quot;^1A&quot;,[115,&quot;^Y&quot;,52,536870981]],[&quot;^1A&quot;,[115,&quot;^?&quot;,&quot;~u6120a4ae-1b59-4bb9-9d97-dfd81849b6c5&quot;,536870983]],[&quot;^1A&quot;,[116,&quot;^S&quot;,[&quot;^1@&quot;,[[&quot;Paragraph&quot;,[[&quot;Break_Line&quot;]]]]],536870981]],[&quot;^1A&quot;,[116,&quot;^[&quot;,&quot;tags:: [[Computer Science - Computation and Language]], [[Computer Science - Machine Learning]], [[Computer Science - Sound]], [[Electrical Engineering and Systems Science - Audio and Speech Processing]]\\ndate:: [[Jun 9th, 2021]]\\nextra:: arXiv: 2104.04298\\ntitle:: Feature Replacement and Combination for Hybrid ASR Systems\\nitem-type:: [[journalArticle]]\\naccess-date:: 2021-07-07T05:39:04Z\\noriginal-title:: Feature Replacement and Combination for Hybrid ASR Systems\\nurl:: http://arxiv.org/abs/2104.04298\\npublication-title:: \\&quot;arXiv:2104.04298 [cs, eess]\\&quot;\\nauthors:: [[Peter Vieting]], [[Christoph Lüscher]], [[Wilfried Michel]], [[Ralf Schlüter]], [[Hermann Ney]]\\nlibrary-catalog:: arXiv.org\\nlinks:: [Local library](zotero://select/library/items/5JFDDHFZ), [Web library](https://www.zotero.org/users/7048753/items/5JFDDHFZ)\\n\\n&quot;,536870981]],[&quot;^1A&quot;,[116,&quot;^W&quot;,&quot;^1F&quot;,536870981]],[&quot;^1A&quot;,[116,&quot;^K&quot;,115,536870981]],[&quot;^1A&quot;,[116,&quot;^X&quot;,1,536870981]],[&quot;^1A&quot;,[116,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1I&quot;,0,&quot;^1J&quot;,823],536870981]],[&quot;^1A&quot;,[116,&quot;^17&quot;,115,536870981]],[&quot;^1A&quot;,[116,&quot;^13&quot;,115,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,24,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,26,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,27,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,46,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,47,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,52,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,106,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,107,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,108,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,109,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,110,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,111,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,115,536870981]],[&quot;^1A&quot;,[116,&quot;^12&quot;,119,536870981]],[&quot;^1A&quot;,[116,&quot;^=&quot;,true,536870981]],[&quot;^1A&quot;,[116,&quot;^D&quot;,[&quot;^ &quot;,&quot;^1R&quot;,[&quot;^1E&quot;,[&quot;Computer Science - Computation and Language&quot;,&quot;Computer Science - Machine Learning&quot;,&quot;Computer Science - Sound&quot;,&quot;Electrical Engineering and Systems Science - Audio and Speech Processing&quot;]],&quot;^1S&quot;,[&quot;^1E&quot;,[&quot;Jun 9th, 2021&quot;]],&quot;^1T&quot;,&quot;arXiv: 2104.04298&quot;,&quot;^1Q&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;,&quot;^1U&quot;,[&quot;^1E&quot;,[&quot;journalArticle&quot;]],&quot;^1V&quot;,&quot;2021-07-07T05:39:04Z&quot;,&quot;^1W&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;,&quot;^1K&quot;,&quot;http://arxiv.org/abs/2104.04298&quot;,&quot;^1X&quot;,[&quot;^1E&quot;,[&quot;arXiv:2104.04298 [cs&quot;,&quot;eess]&quot;]],&quot;^1Y&quot;,[&quot;^1E&quot;,[&quot;Peter Vieting&quot;,&quot;Christoph Lüscher&quot;,&quot;Wilfried Michel&quot;,&quot;Ralf Schlüter&quot;,&quot;Hermann Ney&quot;]],&quot;^1Z&quot;,&quot;arXiv.org&quot;,&quot;^1[&quot;,&quot;[Local library](zotero://select/library/items/5JFDDHFZ), [Web library](https://www.zotero.org/users/7048753/items/5JFDDHFZ)&quot;],536870981]],[&quot;^1A&quot;,[116,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^1R&quot;,&quot;^1S&quot;,&quot;^1T&quot;,&quot;^1Q&quot;,&quot;^1U&quot;,&quot;^1V&quot;,&quot;^1W&quot;,&quot;^1K&quot;,&quot;^1X&quot;,&quot;^1Y&quot;,&quot;^1Z&quot;,&quot;^1[&quot;]],536870981]],[&quot;^1A&quot;,[116,&quot;^1B&quot;,true,536870981]],[&quot;^1A&quot;,[116,&quot;^?&quot;,&quot;~u6120a4ab-48a8-4b9e-b5ab-4e26a0400b60&quot;,536870981]],[&quot;^1A&quot;,[117,&quot;^1C&quot;,&quot;Abstract&quot;,536870981]],[&quot;^1A&quot;,[117,&quot;^S&quot;,[],536870981]],[&quot;^1A&quot;,[117,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536870981]],[&quot;^1A&quot;,[117,&quot;^[&quot;,&quot;## Abstract&quot;,536870981]],[&quot;^1A&quot;,[117,&quot;^W&quot;,&quot;^1F&quot;,536870981]],[&quot;^1A&quot;,[117,&quot;^14&quot;,2,536870981]],[&quot;^1A&quot;,[117,&quot;^K&quot;,116,536870981]],[&quot;^1A&quot;,[117,&quot;^X&quot;,1,536870981]],[&quot;^1A&quot;,[117,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,823,&quot;^1J&quot;,835],536870981]],[&quot;^1A&quot;,[117,&quot;^17&quot;,115,536870981]],[&quot;^1A&quot;,[117,&quot;^13&quot;,115,536870981]],[&quot;^1A&quot;,[117,&quot;^12&quot;,115,536870981]],[&quot;^1A&quot;,[117,&quot;~:block/size&quot;,2,536870981]],[&quot;^1A&quot;,[117,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Abstract&quot;]],536870981]],[&quot;^1A&quot;,[117,&quot;^15&quot;,&quot;~:heading&quot;,536870981]],[&quot;^1A&quot;,[117,&quot;^1B&quot;,false,536870981]],[&quot;^1A&quot;,[117,&quot;^?&quot;,&quot;~u6120a4ab-769a-42f8-8a00-cab9e6e5fd55&quot;,536870981]],[&quot;^1A&quot;,[118,&quot;^1C&quot;,&quot;Acoustic_modeling_of_raw_waveform_and_learning_feature_extractors_as_part_of_the_neural_network_classifier_has_been_the_goal_of_many_studies_in_the_area_of_automatic_speech_recognition_(ASR)-2e-_Recently-2c-_one_line_of_research_has_focused_on_frameworks_that_can_be_pre_trained_on_audio_only_data_in_an_unsupervised_fashion_and_aim_at_improving_downstream_ASR_tasks-2e-_In_this_work-2c-_we_investigate_the_usefulness_of_one_of_these_front_end_frameworks-2c-_namely_wav2vec-2c-_for_hybrid_ASR_systems-2e-_In_addition_to_deploying_a_pre_trained_feature_extractor-2c-_we_explore_how_to_make_use_of_an_existing_acoustic_model_(AM)_trained_on_the_same_task_with_different_features_as_well-2e-_Another_neural_front_end_which_is_only_trained_together_with_the_supervised_ASR_loss_as_well_as_traditional_Gammatone_features_are_applied_for_comparison-2e-_Moreover-2c-_it_is_shown_that_the_AM_can_be_retrofitted_with_i_vectors_for_speaker_adaptation-2e-_Finally-2c-_the_described_features_are_combined_in_order_to_further_advance_the_performance-2e-_With_the_final_best_system-2c-_we_obtain_a_relative_improvement_of_4-25-_and_6-25-_over_our_previous_best_model_on_the_LibriSpeech_test_clean_and_test_other_sets-2e-&quot;,536870981]],[&quot;^1A&quot;,[118,&quot;^S&quot;,[],536870981]],[&quot;^1A&quot;,[118,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536870981]],[&quot;^1A&quot;,[118,&quot;^[&quot;,&quot;Acoustic modeling of raw waveform and learning feature extractors as part of the neural network classifier has been the goal of many studies in the area of automatic speech recognition (ASR). Recently, one line of research has focused on frameworks that can be pre-trained on audio-only data in an unsupervised fashion and aim at improving downstream ASR tasks. In this work, we investigate the usefulness of one of these front-end frameworks, namely wav2vec, for hybrid ASR systems. In addition to deploying a pre-trained feature extractor, we explore how to make use of an existing acoustic model (AM) trained on the same task with different features as well. Another neural front-end which is only trained together with the supervised ASR loss as well as traditional Gammatone features are applied for comparison. Moreover, it is shown that the AM can be retrofitted with i-vectors for speaker adaptation. Finally, the described features are combined in order to further advance the performance. With the final best system, we obtain a relative improvement of 4% and 6% over our previous best model on the LibriSpeech test-clean and test-other sets.&quot;,536870981]],[&quot;^1A&quot;,[118,&quot;^W&quot;,&quot;^1F&quot;,536870981]],[&quot;^1A&quot;,[118,&quot;^K&quot;,117,536870981]],[&quot;^1A&quot;,[118,&quot;^X&quot;,1,536870981]],[&quot;^1A&quot;,[118,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,835,&quot;^1J&quot;,1989],536870981]],[&quot;^1A&quot;,[118,&quot;^17&quot;,115,536870981]],[&quot;^1A&quot;,[118,&quot;^13&quot;,115,536870981]],[&quot;^1A&quot;,[118,&quot;^12&quot;,115,536870981]],[&quot;^1A&quot;,[118,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Acoustic modeling of raw waveform and learning feature extractors as part of the neural network classifier has been the goal of many studies in the area of automatic speech recognition (ASR). Recently, one line of research has focused on frameworks that can be pre-trained on audio-only data in an unsupervised fashion and aim at improving downstream ASR tasks. In this work, we investigate the usefulness of one of these front-end frameworks, namely wav2vec, for hybrid ASR systems. In addition to deploying a pre-trained feature extractor, we explore how to make use of an existing acoustic model (AM) trained on the same task with different features as well. Another neural front-end which is only trained together with the supervised ASR loss as well as traditional Gammatone features are applied for comparison. Moreover, it is shown that the AM can be retrofitted with i-vectors for speaker adaptation. Finally, the described features are combined in order to further advance the performance. With the final best system, we obtain a relative improvement of 4% and 6% over our previous best model on the LibriSpeech test-clean and test-other sets.&quot;]],536870981]],[&quot;^1A&quot;,[118,&quot;^1B&quot;,true,536870981]],[&quot;^1A&quot;,[118,&quot;^?&quot;,&quot;~u6120a4ab-6a90-49fc-96a0-d230bd9e1b93&quot;,536870981]],[&quot;^1A&quot;,[119,&quot;^19&quot;,&quot;arxiv:2104.04298 [cs&quot;,536870981]],[&quot;^1A&quot;,[209,&quot;^Q&quot;,1629529702656,536871043]],[&quot;^1A&quot;,[209,&quot;^E&quot;,false,536871036]],[&quot;^1A&quot;,[209,&quot;^19&quot;,&quot;training asr models by generation of contextual information&quot;,536871036]],[&quot;^1A&quot;,[209,&quot;^1=&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;,536871036]],[&quot;^1A&quot;,[209,&quot;^D&quot;,[&quot;^ &quot;,&quot;^1R&quot;,[&quot;^1@&quot;,[&quot;Computer Science - Computation and Language&quot;,&quot;Computer Science - Machine Learning&quot;,&quot;Computer Science - Sound&quot;,&quot;Electrical Engineering and Systems Science - Audio and Speech Processing&quot;]],&quot;^1S&quot;,[&quot;^1E&quot;,[&quot;Feb 14th, 2020&quot;]],&quot;^1T&quot;,&quot;arXiv: 1910.12367&quot;,&quot;^1Q&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;,&quot;^1U&quot;,[&quot;^1E&quot;,[&quot;journalArticle&quot;]],&quot;^1V&quot;,&quot;2021-08-21T03:18:11Z&quot;,&quot;^1W&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;,&quot;^1K&quot;,&quot;http://arxiv.org/abs/1910.12367&quot;,&quot;^1X&quot;,[&quot;^1E&quot;,[&quot;arXiv:1910.12367 [cs&quot;,&quot;eess]&quot;]],&quot;^1Y&quot;,[&quot;^1E&quot;,[&quot;Yatharth Saraf&quot;,&quot;Jun Liu&quot;,&quot;Geoffrey Zweig&quot;,&quot;Ross Girshick&quot;,&quot;Yongqiang Wang&quot;,&quot;Frank Zhang&quot;,&quot;Abdelrahman Mohamed&quot;,&quot;Fuchun Peng&quot;,&quot;Kritika Singh&quot;,&quot;Dmytro Okhonko&quot;,&quot;Sergey Edunov&quot;]],&quot;^1Z&quot;,&quot;arXiv.org&quot;,&quot;^1[&quot;,&quot;[Local library](zotero://select/library/items/UUNX2N8G), [Web library](https://www.zotero.org/users/7048753/items/UUNX2N8G)&quot;],536871036]],[&quot;^1A&quot;,[209,&quot;^Y&quot;,26,536871036]],[&quot;^1A&quot;,[209,&quot;^Y&quot;,27,536871036]],[&quot;^1A&quot;,[209,&quot;^Y&quot;,46,536871036]],[&quot;^1A&quot;,[209,&quot;^Y&quot;,52,536871036]],[&quot;^1A&quot;,[209,&quot;^G&quot;,1629529793368,536871069]],[&quot;^1A&quot;,[209,&quot;^?&quot;,&quot;~u6120a545-5076-4917-b75e-04c7510f5ed7&quot;,536871036]],[&quot;^1A&quot;,[210,&quot;^S&quot;,[&quot;^1@&quot;,[[&quot;Paragraph&quot;,[[&quot;Break_Line&quot;]]]]],536871036]],[&quot;^1A&quot;,[210,&quot;^[&quot;,&quot;tags:: [[Computer Science - Computation and Language]], [[Computer Science - Machine Learning]], [[Computer Science - Sound]], [[Electrical Engineering and Systems Science - Audio and Speech Processing]]\\ndate:: [[Feb 14th, 2020]]\\nextra:: arXiv: 1910.12367\\ntitle:: Training ASR models by Generation of Contextual Information\\nitem-type:: [[journalArticle]]\\naccess-date:: 2021-08-21T03:18:11Z\\noriginal-title:: Training ASR models by Generation of Contextual Information\\nurl:: http://arxiv.org/abs/1910.12367\\npublication-title:: \\&quot;arXiv:1910.12367 [cs, eess]\\&quot;\\nauthors:: [[Kritika Singh]], [[Dmytro Okhonko]], [[Jun Liu]], [[Yongqiang Wang]], [[Frank Zhang]], [[Ross Girshick]], [[Sergey Edunov]], [[Fuchun Peng]], [[Yatharth Saraf]], [[Geoffrey Zweig]], [[Abdelrahman Mohamed]]\\nlibrary-catalog:: arXiv.org\\nlinks:: [Local library](zotero://select/library/items/UUNX2N8G), [Web library](https://www.zotero.org/users/7048753/items/UUNX2N8G)\\n\\n&quot;,536871036]],[&quot;^1A&quot;,[210,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[210,&quot;^K&quot;,209,536871036]],[&quot;^1A&quot;,[210,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[210,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1I&quot;,0,&quot;^1J&quot;,934],536871036]],[&quot;^1A&quot;,[210,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[210,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,24,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,25,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,26,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,27,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,29,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,30,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,32,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,33,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,34,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,35,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,38,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,41,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,43,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,44,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,46,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,47,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,51,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,52,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,53,536871036]],[&quot;^1A&quot;,[210,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[210,&quot;^=&quot;,true,536871036]],[&quot;^1A&quot;,[210,&quot;^D&quot;,[&quot;^ &quot;,&quot;^1R&quot;,[&quot;^1E&quot;,[&quot;Computer Science - Computation and Language&quot;,&quot;Computer Science - Machine Learning&quot;,&quot;Computer Science - Sound&quot;,&quot;Electrical Engineering and Systems Science - Audio and Speech Processing&quot;]],&quot;^1S&quot;,[&quot;^1E&quot;,[&quot;Feb 14th, 2020&quot;]],&quot;^1T&quot;,&quot;arXiv: 1910.12367&quot;,&quot;^1Q&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;,&quot;^1U&quot;,[&quot;^1E&quot;,[&quot;journalArticle&quot;]],&quot;^1V&quot;,&quot;2021-08-21T03:18:11Z&quot;,&quot;^1W&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;,&quot;^1K&quot;,&quot;http://arxiv.org/abs/1910.12367&quot;,&quot;^1X&quot;,[&quot;^1E&quot;,[&quot;arXiv:1910.12367 [cs&quot;,&quot;eess]&quot;]],&quot;^1Y&quot;,[&quot;^1E&quot;,[&quot;Yatharth Saraf&quot;,&quot;Jun Liu&quot;,&quot;Geoffrey Zweig&quot;,&quot;Ross Girshick&quot;,&quot;Yongqiang Wang&quot;,&quot;Frank Zhang&quot;,&quot;Abdelrahman Mohamed&quot;,&quot;Fuchun Peng&quot;,&quot;Kritika Singh&quot;,&quot;Dmytro Okhonko&quot;,&quot;Sergey Edunov&quot;]],&quot;^1Z&quot;,&quot;arXiv.org&quot;,&quot;^1[&quot;,&quot;[Local library](zotero://select/library/items/UUNX2N8G), [Web library](https://www.zotero.org/users/7048753/items/UUNX2N8G)&quot;],536871036]],[&quot;^1A&quot;,[210,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^1R&quot;,&quot;^1S&quot;,&quot;^1T&quot;,&quot;^1Q&quot;,&quot;^1U&quot;,&quot;^1V&quot;,&quot;^1W&quot;,&quot;^1K&quot;,&quot;^1X&quot;,&quot;^1Y&quot;,&quot;^1Z&quot;,&quot;^1[&quot;]],536871036]],[&quot;^1A&quot;,[210,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[210,&quot;^?&quot;,&quot;~u6120a545-6918-4691-8d1e-2160a6bc4179&quot;,536871036]],[&quot;^1A&quot;,[211,&quot;^1C&quot;,&quot;TLDR&quot;,536871036]],[&quot;^1A&quot;,[211,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[211,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[211,&quot;^[&quot;,&quot;## TLDR&quot;,536871036]],[&quot;^1A&quot;,[211,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[211,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[211,&quot;^K&quot;,210,536871036]],[&quot;^1A&quot;,[211,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[211,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,934,&quot;^1J&quot;,942],536871036]],[&quot;^1A&quot;,[211,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[211,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[211,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[211,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[211,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;TLDR&quot;]],536871036]],[&quot;^1A&quot;,[211,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[211,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[211,&quot;^?&quot;,&quot;~u6120a545-14b9-4d79-b66a-8e7cd3995afa&quot;,536871036]],[&quot;^1A&quot;,[212,&quot;^1C&quot;,&quot;Use_weak_supervision_from_social_media_posts_and_augment_it_with_supervised_training_for_surpassing_supervised_only_training_results-2e-_3_main_scenarios___clean-2c-_noisy_and_extreme___are_used_to_show_results-2e-&quot;,536871036]],[&quot;^1A&quot;,[212,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[212,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[212,&quot;^[&quot;,&quot;Use weak supervision from social media posts and augment it with supervised training for surpassing supervised only training results. 3 main scenarios - clean, noisy and _extreme_ - are used to show results.&quot;,536871036]],[&quot;^1A&quot;,[212,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[212,&quot;^K&quot;,211,536871036]],[&quot;^1A&quot;,[212,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[212,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,942,&quot;^1J&quot;,1152],536871036]],[&quot;^1A&quot;,[212,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[212,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[212,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[212,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Use weak supervision from social media posts and augment it with supervised training for surpassing supervised only training results. 3 main scenarios - clean, noisy and &quot;],[&quot;Emphasis&quot;,[[&quot;Italic&quot;],[[&quot;Plain&quot;,&quot;extreme&quot;]]]],[&quot;Plain&quot;,&quot; - are used to show results.&quot;]],536871036]],[&quot;^1A&quot;,[212,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[212,&quot;^?&quot;,&quot;~u6120a545-07a5-412a-b34b-2580c1800183&quot;,536871036]],[&quot;^1A&quot;,[213,&quot;^1C&quot;,&quot;Unique&quot;,536871036]],[&quot;^1A&quot;,[213,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[213,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[213,&quot;^[&quot;,&quot;## Unique&quot;,536871036]],[&quot;^1A&quot;,[213,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[213,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[213,&quot;^K&quot;,212,536871036]],[&quot;^1A&quot;,[213,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[213,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1152,&quot;^1J&quot;,1162],536871036]],[&quot;^1A&quot;,[213,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[213,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[213,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[213,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[213,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Unique&quot;]],536871036]],[&quot;^1A&quot;,[213,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[213,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[213,&quot;^?&quot;,&quot;~u6120a545-dc37-45c4-abef-d40844c370ab&quot;,536871036]],[&quot;^1A&quot;,[214,&quot;^1C&quot;,&quot;Use_of_a_starting__stage_in_training_with_supervised_data_to_properly_initialise_weights-2e-_The__phase_uses_a_mix_of_both_supervised_and_weak_supervised_loss_functions-2e-_A_final_supervised_only__phase_to_polish_-22-decoder_cross_attention_input_audio_sequence_to_be_more_monotonic-22--2e-&quot;,536871036]],[&quot;^1A&quot;,[214,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[214,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[214,&quot;^[&quot;,&quot;Use of a starting `burn-in` stage in training with supervised data to properly initialise weights. The `train-main` phase uses a mix of both supervised and weak supervised loss functions. A final supervised-only `fine-tune` phase to polish \\&quot;decoder cross-attention input audio sequence to be more monotonic\\&quot;.\\nid:: 61208c0d-a528-401a-a53d-6e2a88568b1f&quot;,536871036]],[&quot;^1A&quot;,[214,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[214,&quot;^K&quot;,213,536871036]],[&quot;^1A&quot;,[214,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[214,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1162,&quot;^1J&quot;,1517],536871036]],[&quot;^1A&quot;,[214,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[214,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[214,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[214,&quot;^D&quot;,[&quot;^ &quot;,&quot;~:id&quot;,&quot;61208c0d-a528-401a-a53d-6e2a88568b1f&quot;],536871036]],[&quot;^1A&quot;,[214,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^22&quot;]],536871036]],[&quot;^1A&quot;,[214,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Use of a starting &quot;],[&quot;Code&quot;,&quot;burn-in&quot;],[&quot;Plain&quot;,&quot; stage in training with supervised data to properly initialise weights. The &quot;],[&quot;Code&quot;,&quot;train-main&quot;],[&quot;Plain&quot;,&quot; phase uses a mix of both supervised and weak supervised loss functions. A final supervised-only &quot;],[&quot;Code&quot;,&quot;fine-tune&quot;],[&quot;Plain&quot;,&quot; phase to polish \\&quot;decoder cross-attention input audio sequence to be more monotonic\\&quot;.&quot;]],536871036]],[&quot;^1A&quot;,[214,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[214,&quot;^?&quot;,&quot;~u61208c0d-a528-401a-a53d-6e2a88568b1f&quot;,536871036]],[&quot;^1A&quot;,[215,&quot;^1C&quot;,&quot;Main_Points&quot;,536871036]],[&quot;^1A&quot;,[215,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[215,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[215,&quot;^[&quot;,&quot;## Main Points&quot;,536871036]],[&quot;^1A&quot;,[215,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[215,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[215,&quot;^K&quot;,214,536871036]],[&quot;^1A&quot;,[215,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[215,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1517,&quot;^1J&quot;,1532],536871036]],[&quot;^1A&quot;,[215,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[215,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[215,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[215,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[215,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Main Points&quot;]],536871036]],[&quot;^1A&quot;,[215,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[215,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[215,&quot;^?&quot;,&quot;~u6120a545-cd78-4fd4-b8ad-08272e381d36&quot;,536871036]],[&quot;^1A&quot;,[216,&quot;^1C&quot;,&quot;Two_main_assumptions_regarding_unlabelled_data_(D_w)_and_supervised_data_(D_s)_are_assumed-2e-_Y_w_denotes_the_generated_text_sequence-2c-_Y_s_denotes_actual_audio_content-2c-_X_denotes_input_audio_features-2e-&quot;,536871036]],[&quot;^1A&quot;,[216,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[216,&quot;^1D&quot;,[&quot;^1E&quot;,[[&quot;^?&quot;,&quot;~u6120a545-33d5-4a11-bcd6-faa5ee0f65c1&quot;],[&quot;^?&quot;,&quot;~u6120a545-12cf-42b0-a83c-e673cf2fad22&quot;]]],536871036]],[&quot;^1A&quot;,[216,&quot;^[&quot;,&quot;Two main assumptions regarding unlabelled data (D_w) and supervised data (D_s) are assumed. Y_w denotes the generated text sequence, Y_s denotes actual audio content, X denotes input audio features.\\nid:: 61208e0f-900e-4e11-9fb7-fa0311ed09ce&quot;,536871036]],[&quot;^1A&quot;,[216,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[216,&quot;^K&quot;,215,536871036]],[&quot;^1A&quot;,[216,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[216,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1532,&quot;^1J&quot;,1777],536871036]],[&quot;^1A&quot;,[216,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[216,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[216,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[216,&quot;^D&quot;,[&quot;^ &quot;,&quot;^22&quot;,&quot;61208e0f-900e-4e11-9fb7-fa0311ed09ce&quot;],536871036]],[&quot;^1A&quot;,[216,&quot;^P&quot;,[&quot;^1@&quot;,[&quot;^22&quot;]],536871036]],[&quot;^1A&quot;,[216,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Two main assumptions regarding unlabelled data (D_w) and supervised data (D_s) are assumed. Y_w denotes the generated text sequence, Y_s denotes actual audio content, X denotes input audio features.&quot;]],536871036]],[&quot;^1A&quot;,[216,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[216,&quot;^?&quot;,&quot;~u61208e0f-900e-4e11-9fb7-fa0311ed09ce&quot;,536871036]],[&quot;^1A&quot;,[217,&quot;^1C&quot;,&quot;(i)_amount_of_D_w_much_higher_than_D_s-2c-_i-2e-e-2e-_-7c-D_w-7c-_-3e--3e-_-7c-D_s-7c--2e-_Also_D_w_is_more_acoustically_diverse_with_many_more_speakers_than_D_s-2e-&quot;,536871036]],[&quot;^1A&quot;,[217,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[217,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[217,&quot;^[&quot;,&quot;(i) amount of D_w much higher than D_s, i.e. |D_w| &gt;&gt; |D_s|. Also D_w is more acoustically diverse with many more speakers than D_s.&quot;,536871036]],[&quot;^1A&quot;,[217,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[217,&quot;^K&quot;,216,536871036]],[&quot;^1A&quot;,[217,&quot;^X&quot;,2,536871036]],[&quot;^1A&quot;,[217,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1777,&quot;^1J&quot;,1913],536871036]],[&quot;^1A&quot;,[217,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[217,&quot;^13&quot;,216,536871036]],[&quot;^1A&quot;,[217,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[217,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;(i) amount of D_w much higher than D_s, i.e. |D_w| &gt;&gt; |D_s|. Also D_w is more acoustically diverse with many more speakers than D_s.&quot;]],536871036]],[&quot;^1A&quot;,[217,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[217,&quot;^?&quot;,&quot;~u6120a545-12cf-42b0-a83c-e673cf2fad22&quot;,536871036]],[&quot;^1A&quot;,[218,&quot;^1C&quot;,&quot;(ii)_maximising_p(Y_w-7c-X-3b--5c-theta)_is_a_good_enough_proxy_to_maximise_p(Y_s-7c-X-3b--5c-theta)-2e-_Several_training_configurations_are_used_to_validate_this_assumption-2e-&quot;,536871036]],[&quot;^1A&quot;,[218,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[218,&quot;^1D&quot;,[&quot;^1E&quot;,[[&quot;^?&quot;,&quot;~u6120a545-89f3-4a99-a3d8-20686f8e0a88&quot;],[&quot;^?&quot;,&quot;~u6120a545-f822-4fcc-a13c-9cc15fecc97d&quot;],[&quot;^?&quot;,&quot;~u6120a545-6b7e-418b-b613-df3e9d78b79e&quot;]]],536871036]],[&quot;^1A&quot;,[218,&quot;^[&quot;,&quot;(ii) maximising p(Y_w|X;$\\\\theta$) is a good enough proxy to maximise p(Y_s|X;$\\\\theta$). Several training configurations are used to validate this assumption.&quot;,536871036]],[&quot;^1A&quot;,[218,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[218,&quot;^K&quot;,217,536871036]],[&quot;^1A&quot;,[218,&quot;^X&quot;,2,536871036]],[&quot;^1A&quot;,[218,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,1913,&quot;^1J&quot;,2074],536871036]],[&quot;^1A&quot;,[218,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[218,&quot;^13&quot;,216,536871036]],[&quot;^1A&quot;,[218,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[218,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;(ii) maximising p(Y_w|X;&quot;],[&quot;Latex_Fragment&quot;,[&quot;Inline&quot;,&quot;\\\\theta&quot;]],[&quot;Plain&quot;,&quot;) is a good enough proxy to maximise p(Y_s|X;&quot;],[&quot;Latex_Fragment&quot;,[&quot;Inline&quot;,&quot;\\\\theta&quot;]],[&quot;Plain&quot;,&quot;). Several training configurations are used to validate this assumption.&quot;]],536871036]],[&quot;^1A&quot;,[218,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[218,&quot;^?&quot;,&quot;~u6120a545-33d5-4a11-bcd6-faa5ee0f65c1&quot;,536871036]],[&quot;^1A&quot;,[219,&quot;^1C&quot;,&quot;How_do_we_estimate_similarity_between_Y_s_and_Y_w-3f-_Here_Y_s_is_generated_using_a_baseline_ASR_system-2e-_A_set_intersection_of_words_between_Y_s_and_Y_w_acts_as_a_measure_of_relatedness_between_audio_content_and_weak_labels-2e-&quot;,536871036]],[&quot;^1A&quot;,[219,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[219,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[219,&quot;^[&quot;,&quot;**How do we estimate similarity between Y_s and Y_w?** Here Y_s is generated using a baseline ASR system. _A set intersection of words between Y_s and Y_w acts as a measure of relatedness between audio content and weak labels._&quot;,536871036]],[&quot;^1A&quot;,[219,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[219,&quot;^K&quot;,218,536871036]],[&quot;^1A&quot;,[219,&quot;^X&quot;,3,536871036]],[&quot;^1A&quot;,[219,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2074,&quot;^1J&quot;,2306],536871036]],[&quot;^1A&quot;,[219,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[219,&quot;^13&quot;,218,536871036]],[&quot;^1A&quot;,[219,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[219,&quot;^Z&quot;,[[&quot;Emphasis&quot;,[[&quot;Bold&quot;],[[&quot;Plain&quot;,&quot;How do we estimate similarity between Y_s and Y_w?&quot;]]]],[&quot;Plain&quot;,&quot; Here Y_s is generated using a baseline ASR system. &quot;],[&quot;Emphasis&quot;,[[&quot;Italic&quot;],[[&quot;Plain&quot;,&quot;A set intersection of words between Y_s and Y_w acts as a measure of relatedness between audio content and weak labels.&quot;]]]]],536871036]],[&quot;^1A&quot;,[219,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[219,&quot;^?&quot;,&quot;~u6120a545-6b7e-418b-b613-df3e9d78b79e&quot;,536871036]],[&quot;^1A&quot;,[220,&quot;^1C&quot;,&quot;maximising_p(Y_w-7c-X-3b-)_improves_p(Y_s-7c-X-3b-)_during_all_phases_of_training-3f-_Authors_study_the_effect_and_conduct_model_training_in_3_different_stages-2c-_as_mentioned_above-2e-&quot;,536871036]],[&quot;^1A&quot;,[220,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[220,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[220,&quot;^[&quot;,&quot;maximising p(Y_w|X;$$\\\\theta$$) improves p(Y_s|X;$$\\\\theta$$) during all phases of training? Authors study the effect and conduct model training in 3 different stages, as mentioned above.&quot;,536871036]],[&quot;^1A&quot;,[220,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[220,&quot;^K&quot;,219,536871036]],[&quot;^1A&quot;,[220,&quot;^X&quot;,3,536871036]],[&quot;^1A&quot;,[220,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2306,&quot;^1J&quot;,2496],536871036]],[&quot;^1A&quot;,[220,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[220,&quot;^13&quot;,218,536871036]],[&quot;^1A&quot;,[220,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[220,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;maximising p(Y_w|X;&quot;],[&quot;Latex_Fragment&quot;,[&quot;Displayed&quot;,&quot;\\\\theta&quot;]],[&quot;Plain&quot;,&quot;) improves p(Y_s|X;&quot;],[&quot;Latex_Fragment&quot;,[&quot;Displayed&quot;,&quot;\\\\theta&quot;]],[&quot;Plain&quot;,&quot;) during all phases of training? Authors study the effect and conduct model training in 3 different stages, as mentioned above.&quot;]],536871036]],[&quot;^1A&quot;,[220,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[220,&quot;^?&quot;,&quot;~u6120a545-f822-4fcc-a13c-9cc15fecc97d&quot;,536871036]],[&quot;^1A&quot;,[221,&quot;^1C&quot;,&quot;Will_such_training_benefit_AM_-2f-_LM_components_separately_or_just_E2E_model-3f-_Authors_show_results_where_just__is_used_for__with_a_CTC_loss_which_also_shows_improvements-2e-&quot;,536871036]],[&quot;^1A&quot;,[221,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[221,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[221,&quot;^[&quot;,&quot;**Will such training benefit AM / LM components separately or just E2E model?** Authors show results where just $$\\\\theta _{enc}$$ is used for `fine-tune` with a CTC loss which also shows improvements.&quot;,536871036]],[&quot;^1A&quot;,[221,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[221,&quot;^K&quot;,220,536871036]],[&quot;^1A&quot;,[221,&quot;^X&quot;,3,536871036]],[&quot;^1A&quot;,[221,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2496,&quot;^1J&quot;,2701],536871036]],[&quot;^1A&quot;,[221,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[221,&quot;^13&quot;,218,536871036]],[&quot;^1A&quot;,[221,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[221,&quot;^Z&quot;,[[&quot;Emphasis&quot;,[[&quot;Bold&quot;],[[&quot;Plain&quot;,&quot;Will such training benefit AM / LM components separately or just E2E model?&quot;]]]],[&quot;Plain&quot;,&quot; Authors show results where just &quot;],[&quot;Latex_Fragment&quot;,[&quot;Displayed&quot;,&quot;\\\\theta _{enc}&quot;]],[&quot;Plain&quot;,&quot; is used for &quot;],[&quot;Code&quot;,&quot;fine-tune&quot;],[&quot;Plain&quot;,&quot; with a CTC loss which also shows improvements.&quot;]],536871036]],[&quot;^1A&quot;,[221,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[221,&quot;^?&quot;,&quot;~u6120a545-89f3-4a99-a3d8-20686f8e0a88&quot;,536871036]],[&quot;^1A&quot;,[222,&quot;^1C&quot;,&quot;Training&quot;,536871036]],[&quot;^1A&quot;,[222,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[222,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[222,&quot;^[&quot;,&quot;## Training&quot;,536871036]],[&quot;^1A&quot;,[222,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[222,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[222,&quot;^K&quot;,216,536871036]],[&quot;^1A&quot;,[222,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[222,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2701,&quot;^1J&quot;,2713],536871036]],[&quot;^1A&quot;,[222,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[222,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[222,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[222,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[222,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Training&quot;]],536871036]],[&quot;^1A&quot;,[222,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[222,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[222,&quot;^?&quot;,&quot;~u6120a545-60eb-4452-9578-b4015a2a9f10&quot;,536871036]],[&quot;^1A&quot;,[223,&quot;^1C&quot;,&quot;During__phase-2c-_mini_batches_alternated_between_D_w_and_D_s-2e-&quot;,536871036]],[&quot;^1A&quot;,[223,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[223,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[223,&quot;^[&quot;,&quot;During `train-main` phase, mini-batches alternated between D_w and D_s.&quot;,536871036]],[&quot;^1A&quot;,[223,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[223,&quot;^K&quot;,222,536871036]],[&quot;^1A&quot;,[223,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[223,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2713,&quot;^1J&quot;,2787],536871036]],[&quot;^1A&quot;,[223,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[223,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[223,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[223,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;During &quot;],[&quot;Code&quot;,&quot;train-main&quot;],[&quot;Plain&quot;,&quot; phase, mini-batches alternated between D_w and D_s.&quot;]],536871036]],[&quot;^1A&quot;,[223,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[223,&quot;^?&quot;,&quot;~u6120a545-057d-43a1-9c77-f758fc699a44&quot;,536871036]],[&quot;^1A&quot;,[224,&quot;^1C&quot;,&quot;Input_features_are_regular_80_dimensions_mel_scale_log_filter_bank_features___computed_over_16ms_with_10ms_shift-2e-&quot;,536871036]],[&quot;^1A&quot;,[224,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[224,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[224,&quot;^[&quot;,&quot;Input features are regular 80 dimensions mel-scale log filter bank features - computed over 16ms with 10ms shift.&quot;,536871036]],[&quot;^1A&quot;,[224,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[224,&quot;^K&quot;,223,536871036]],[&quot;^1A&quot;,[224,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[224,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2787,&quot;^1J&quot;,2903],536871036]],[&quot;^1A&quot;,[224,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[224,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[224,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[224,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Input features are regular 80 dimensions mel-scale log filter bank features - computed over 16ms with 10ms shift.&quot;]],536871036]],[&quot;^1A&quot;,[224,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[224,&quot;^?&quot;,&quot;~u6120a545-3ef6-4aa3-89de-c1f7c3f22dba&quot;,536871036]],[&quot;^1A&quot;,[225,&quot;^1C&quot;,&quot;Use_AdaDelta_for_training_with__and_gradient_clipping_at_10-2e-0_where_total_gradients_are_scaled_by_the_number_of_utterances_in_each_minibatch-2e-&quot;,536871036]],[&quot;^1A&quot;,[225,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[225,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[225,&quot;^[&quot;,&quot;Use AdaDelta for training with `fixed lr=1.0` and gradient clipping at 10.0 where total gradients are scaled by the number of utterances in each minibatch.&quot;,536871036]],[&quot;^1A&quot;,[225,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[225,&quot;^K&quot;,224,536871036]],[&quot;^1A&quot;,[225,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[225,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,2903,&quot;^1J&quot;,3061],536871036]],[&quot;^1A&quot;,[225,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[225,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[225,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[225,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Use AdaDelta for training with &quot;],[&quot;Code&quot;,&quot;fixed lr=1.0&quot;],[&quot;Plain&quot;,&quot; and gradient clipping at 10.0 where total gradients are scaled by the number of utterances in each minibatch.&quot;]],536871036]],[&quot;^1A&quot;,[225,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[225,&quot;^?&quot;,&quot;~u6120a545-3db1-45c5-b587-9b63308579c5&quot;,536871036]],[&quot;^1A&quot;,[226,&quot;^1C&quot;,&quot;Explanation_for_using_a__phase_in_training&quot;,536871036]],[&quot;^1A&quot;,[226,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[226,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[226,&quot;^[&quot;,&quot;## Explanation for using a `burn-in` phase in training&quot;,536871036]],[&quot;^1A&quot;,[226,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[226,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[226,&quot;^K&quot;,225,536871036]],[&quot;^1A&quot;,[226,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[226,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3061,&quot;^1J&quot;,3116],536871036]],[&quot;^1A&quot;,[226,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[226,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[226,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[226,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[226,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Explanation for using a &quot;],[&quot;Code&quot;,&quot;burn-in&quot;],[&quot;Plain&quot;,&quot; phase in training&quot;]],536871036]],[&quot;^1A&quot;,[226,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[226,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[226,&quot;^?&quot;,&quot;~u6120a545-e97c-4dd9-9193-4b0a3a5aceb1&quot;,536871036]],[&quot;^1A&quot;,[227,&quot;^1C&quot;,&quot;image-2e-png&quot;,536871036]],[&quot;^1A&quot;,[227,&quot;^S&quot;,[],536871043]],[&quot;^1A&quot;,[227,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[227,&quot;^[&quot;,&quot;![image.png](../assets/image_1629525676888_0.png)&quot;,536871036]],[&quot;^1A&quot;,[227,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[227,&quot;^K&quot;,226,536871036]],[&quot;^1A&quot;,[227,&quot;^X&quot;,1,536871043]],[&quot;^1A&quot;,[227,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3116,&quot;^1J&quot;,3168],536871036]],[&quot;^1A&quot;,[227,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[227,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[227,&quot;^12&quot;,209,536871043]],[&quot;^1A&quot;,[227,&quot;^D&quot;,[&quot;^ &quot;],536871043]],[&quot;^1A&quot;,[227,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;^1K&quot;,[&quot;Search&quot;,&quot;../assets/image_1629525676888_0.png&quot;],&quot;^1N&quot;,[[&quot;Plain&quot;,&quot;image.png&quot;]],&quot;^1O&quot;,&quot;![image.png](../assets/image_1629525676888_0.png)&quot;,&quot;^1P&quot;,&quot;&quot;]]],536871043]],[&quot;^1A&quot;,[227,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[227,&quot;^?&quot;,&quot;~u6120a545-683e-4d6c-8d1d-43d2b5937d21&quot;,536871036]],[&quot;^1A&quot;,[230,&quot;^1C&quot;,&quot;Results&quot;,536871036]],[&quot;^1A&quot;,[230,&quot;^S&quot;,[],536871063]],[&quot;^1A&quot;,[230,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[230,&quot;^[&quot;,&quot;## Results&quot;,536871036]],[&quot;^1A&quot;,[230,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[230,&quot;^14&quot;,2,536871063]],[&quot;^1A&quot;,[230,&quot;^K&quot;,227,536871063]],[&quot;^1A&quot;,[230,&quot;^X&quot;,1,536871063]],[&quot;^1A&quot;,[230,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3531,&quot;^1J&quot;,3542],536871036]],[&quot;^1A&quot;,[230,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[230,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[230,&quot;^12&quot;,209,536871063]],[&quot;^1A&quot;,[230,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[230,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Results&quot;]],536871063]],[&quot;^1A&quot;,[230,&quot;^15&quot;,&quot;^21&quot;,536871063]],[&quot;^1A&quot;,[230,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[230,&quot;^?&quot;,&quot;~u6120a545-2f4c-4220-b567-fb81a6728a74&quot;,536871036]],[&quot;^1A&quot;,[231,&quot;^1C&quot;,&quot;image-2e-png&quot;,536871036]],[&quot;^1A&quot;,[231,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[231,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[231,&quot;^[&quot;,&quot;![image.png](../assets/image_1629524983473_0.png)&quot;,536871036]],[&quot;^1A&quot;,[231,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[231,&quot;^K&quot;,230,536871036]],[&quot;^1A&quot;,[231,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[231,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3542,&quot;^1J&quot;,3594],536871036]],[&quot;^1A&quot;,[231,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[231,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[231,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[231,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;^1K&quot;,[&quot;Search&quot;,&quot;../assets/image_1629524983473_0.png&quot;],&quot;^1N&quot;,[[&quot;Plain&quot;,&quot;image.png&quot;]],&quot;^1O&quot;,&quot;![image.png](../assets/image_1629524983473_0.png)&quot;,&quot;^1P&quot;,&quot;&quot;]]],536871036]],[&quot;^1A&quot;,[231,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[231,&quot;^?&quot;,&quot;~u6120a545-5343-4f95-ac17-236300e70abf&quot;,536871036]],[&quot;^1A&quot;,[232,&quot;^1C&quot;,&quot;__have_both_encoder_and_decoder_trained_in_E2E_manner-2e-____use__only_and__with_CTC_loss-2e-&quot;,536871036]],[&quot;^1A&quot;,[232,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[232,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[232,&quot;^[&quot;,&quot;~`Encoder-Decoder` - have both encoder and decoder trained in E2E manner. `CTC` - use $$\\\\theta_{enc}$$ only and `fine-tune` with CTC loss.&quot;,536871036]],[&quot;^1A&quot;,[232,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[232,&quot;^K&quot;,231,536871036]],[&quot;^1A&quot;,[232,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[232,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3594,&quot;^1J&quot;,3734],536871036]],[&quot;^1A&quot;,[232,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[232,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[232,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[232,&quot;^Z&quot;,[[&quot;Code&quot;,&quot;Encoder-Decoder&quot;],[&quot;Plain&quot;,&quot; - have both encoder and decoder trained in E2E manner. &quot;],[&quot;Code&quot;,&quot;CTC&quot;],[&quot;Plain&quot;,&quot; - use &quot;],[&quot;Latex_Fragment&quot;,[&quot;Displayed&quot;,&quot;\\\\theta_{enc}&quot;]],[&quot;Plain&quot;,&quot; only and &quot;],[&quot;Code&quot;,&quot;fine-tune&quot;],[&quot;Plain&quot;,&quot; with CTC loss.&quot;]],536871036]],[&quot;^1A&quot;,[232,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[232,&quot;^?&quot;,&quot;~u6120a545-2b0b-476d-b5db-f612eba3d413&quot;,536871036]],[&quot;^1A&quot;,[233,&quot;^1C&quot;,&quot;Even_when_using_additional_2000_hrs_of_supervised_data-2c-_best_weakly_supervised_models_are_consistently_better-2e-_It_might_be_due_to_assumption_(i)-2c-_i-2e-e-2e-_more_diversity_of_acoustic_features_and_speakers_present_in_D_w_than_in_additional_D_s-2e-&quot;,536871036]],[&quot;^1A&quot;,[233,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[233,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[233,&quot;^[&quot;,&quot;Even when using additional 2000 hrs of supervised data, best weakly supervised models are consistently better. It might be due to assumption (i), i.e. more diversity of acoustic features and speakers present in D_w than in additional D_s.&quot;,536871036]],[&quot;^1A&quot;,[233,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[233,&quot;^K&quot;,232,536871036]],[&quot;^1A&quot;,[233,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[233,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3734,&quot;^1J&quot;,3975],536871036]],[&quot;^1A&quot;,[233,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[233,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[233,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[233,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Even when using additional 2000 hrs of supervised data, best weakly supervised models are consistently better. It might be due to assumption (i), i.e. more diversity of acoustic features and speakers present in D_w than in additional D_s.&quot;]],536871036]],[&quot;^1A&quot;,[233,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[233,&quot;^?&quot;,&quot;~u6120a545-0ebc-4d41-b7fd-aae1236a6141&quot;,536871036]],[&quot;^1A&quot;,[234,&quot;^1C&quot;,&quot;&quot;,536871036]],[&quot;^1A&quot;,[234,&quot;^S&quot;,[[&quot;Custom&quot;,&quot;note&quot;,null,[[&quot;Paragraph&quot;,[[&quot;Plain&quot;,&quot;A consistent observation with unsupervised training has been the importance of data processing on audio to ensure diversity of acoustic features and speakers. The paper highlights the importance of data filtering with weak supervision as well.&quot;],[&quot;Break_Line&quot;]]]],&quot;A consistent observation with unsupervised training has been the importance of data processing on audio to ensure diversity of acoustic features and speakers. The paper highlights the importance of data filtering with weak supervision as well.\\n&quot;]],536871069]],[&quot;^1A&quot;,[234,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[234,&quot;^[&quot;,&quot;#+BEGIN_NOTE\\nA consistent observation with unsupervised training has been the importance of data processing on audio to ensure diversity of acoustic features and speakers. The paper highlights the importance of data filtering with weak supervision as well.\\n#+END_NOTE&quot;,536871069]],[&quot;^1A&quot;,[234,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[234,&quot;^K&quot;,233,536871036]],[&quot;^1A&quot;,[234,&quot;^X&quot;,1,536871069]],[&quot;^1A&quot;,[234,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,3975,&quot;^1J&quot;,4256],536871036]],[&quot;^1A&quot;,[234,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[234,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[234,&quot;^12&quot;,209,536871069]],[&quot;^1A&quot;,[234,&quot;^D&quot;,[&quot;^ &quot;],536871069]],[&quot;^1A&quot;,[234,&quot;^Z&quot;,[],536871069]],[&quot;^1A&quot;,[234,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[234,&quot;^?&quot;,&quot;~u6120a545-dd03-48b1-9163-73c530c9f0ad&quot;,536871036]],[&quot;^1A&quot;,[235,&quot;^1C&quot;,&quot;image-2e-png&quot;,536871036]],[&quot;^1A&quot;,[235,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[235,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[235,&quot;^[&quot;,&quot;![image.png](../assets/image_1629526367648_0.png)&quot;,536871036]],[&quot;^1A&quot;,[235,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[235,&quot;^K&quot;,234,536871036]],[&quot;^1A&quot;,[235,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[235,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,4256,&quot;^1J&quot;,4308],536871036]],[&quot;^1A&quot;,[235,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[235,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[235,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[235,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;^1K&quot;,[&quot;Search&quot;,&quot;../assets/image_1629526367648_0.png&quot;],&quot;^1N&quot;,[[&quot;Plain&quot;,&quot;image.png&quot;]],&quot;^1O&quot;,&quot;![image.png](../assets/image_1629526367648_0.png)&quot;,&quot;^1P&quot;,&quot;&quot;]]],536871036]],[&quot;^1A&quot;,[235,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[235,&quot;^?&quot;,&quot;~u6120a545-f1ac-4090-bcb7-5d996398222a&quot;,536871036]],[&quot;^1A&quot;,[236,&quot;^1C&quot;,&quot;Abstract&quot;,536871036]],[&quot;^1A&quot;,[236,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[236,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[236,&quot;^[&quot;,&quot;## Abstract&quot;,536871036]],[&quot;^1A&quot;,[236,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[236,&quot;^14&quot;,2,536871036]],[&quot;^1A&quot;,[236,&quot;^K&quot;,235,536871036]],[&quot;^1A&quot;,[236,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[236,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,4308,&quot;^1J&quot;,4320],536871036]],[&quot;^1A&quot;,[236,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[236,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[236,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[236,&quot;^20&quot;,2,536871036]],[&quot;^1A&quot;,[236,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Abstract&quot;]],536871036]],[&quot;^1A&quot;,[236,&quot;^15&quot;,&quot;^21&quot;,536871036]],[&quot;^1A&quot;,[236,&quot;^1B&quot;,false,536871036]],[&quot;^1A&quot;,[236,&quot;^?&quot;,&quot;~u6120a545-94d1-4ea0-889e-d208d1cd855d&quot;,536871036]],[&quot;^1A&quot;,[237,&quot;^1C&quot;,&quot;Supervised_ASR_models_have_reached_unprecedented_levels_of_accuracy-2c-_thanks_in_part_to_ever_increasing_amounts_of_labelled_training_data-2e-_However-2c-_in_many_applications_and_locales-2c-_only_moderate_amounts_of_data_are_available-2c-_which_has_led_to_a_surge_in_semi__and_weakly_supervised_learning_research-2e-_In_this_paper-2c-_we_conduct_a_large_scale_study_evaluating_the_effectiveness_of_weakly_supervised_learning_for_speech_recognition_by_using_loosely_related_contextual_information_as_a_surrogate_for_ground_truth_labels-2e-_For_weakly_supervised_training-2c-_we_use_50k_hours_of_public_English_social_media_videos_along_with_their_respective_titles_and_post_text_to_train_an_encoder_decoder_transformer_model-2e-_Our_best_encoder_decoder_models_achieve_an_average_of_20-2e-8-25-_WER_reduction_over_a_1000_hours_supervised_baseline-2c-_and_an_average_of_13-2e-4-25-_WER_reduction_when_using_only_the_weakly_supervised_encoder_for_CTC_fine_tuning-2e-_Our_results_show_that_our_setup_for_weak_supervision_improved_both_the_encoder_acoustic_representations_as_well_as_the_decoder_language_generation_abilities-2e-&quot;,536871036]],[&quot;^1A&quot;,[237,&quot;^S&quot;,[],536871036]],[&quot;^1A&quot;,[237,&quot;^1D&quot;,[&quot;^1E&quot;,[]],536871036]],[&quot;^1A&quot;,[237,&quot;^[&quot;,&quot;Supervised ASR models have reached unprecedented levels of accuracy, thanks in part to ever-increasing amounts of labelled training data. However, in many applications and locales, only moderate amounts of data are available, which has led to a surge in semi- and weakly-supervised learning research. In this paper, we conduct a large-scale study evaluating the effectiveness of weakly-supervised learning for speech recognition by using loosely related contextual information as a surrogate for ground-truth labels. For weakly supervised training, we use 50k hours of public English social media videos along with their respective titles and post text to train an encoder-decoder transformer model. Our best encoder-decoder models achieve an average of 20.8% WER reduction over a 1000 hours supervised baseline, and an average of 13.4% WER reduction when using only the weakly supervised encoder for CTC fine-tuning. Our results show that our setup for weak supervision improved both the encoder acoustic representations as well as the decoder language generation abilities.&quot;,536871036]],[&quot;^1A&quot;,[237,&quot;^W&quot;,&quot;^1F&quot;,536871036]],[&quot;^1A&quot;,[237,&quot;^K&quot;,236,536871036]],[&quot;^1A&quot;,[237,&quot;^X&quot;,1,536871036]],[&quot;^1A&quot;,[237,&quot;^U&quot;,[&quot;^ &quot;,&quot;^1G&quot;,[],&quot;^1H&quot;,[],&quot;^1I&quot;,4320,&quot;^1J&quot;,5397],536871036]],[&quot;^1A&quot;,[237,&quot;^17&quot;,209,536871036]],[&quot;^1A&quot;,[237,&quot;^13&quot;,209,536871036]],[&quot;^1A&quot;,[237,&quot;^12&quot;,209,536871036]],[&quot;^1A&quot;,[237,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Supervised ASR models have reached unprecedented levels of accuracy, thanks in part to ever-increasing amounts of labelled training data. However, in many applications and locales, only moderate amounts of data are available, which has led to a surge in semi- and weakly-supervised learning research. In this paper, we conduct a large-scale study evaluating the effectiveness of weakly-supervised learning for speech recognition by using loosely related contextual information as a surrogate for ground-truth labels. For weakly supervised training, we use 50k hours of public English social media videos along with their respective titles and post text to train an encoder-decoder transformer model. Our best encoder-decoder models achieve an average of 20.8% WER reduction over a 1000 hours supervised baseline, and an average of 13.4% WER reduction when using only the weakly supervised encoder for CTC fine-tuning. Our results show that our setup for weak supervision improved both the encoder acoustic representations as well as the decoder language generation abilities.&quot;]],536871036]],[&quot;^1A&quot;,[237,&quot;^1B&quot;,true,536871036]],[&quot;^1A&quot;,[237,&quot;^?&quot;,&quot;~u6120a545-1e6f-4c8b-b54c-54ab72ac33d0&quot;,536871036]],[&quot;^1A&quot;,[238,&quot;^1C&quot;,&quot;Issue_with_weak_supervision_in_encoder_decoder_setting_is_that_decoder_is_not_able_to_refine_encoder_representations_easily-2c-_since_the_weak_labels_are_not_aligned_with_input_sequence-2e-&quot;,536871055]],[&quot;^1A&quot;,[238,&quot;^S&quot;,[],536871058]],[&quot;^1A&quot;,[238,&quot;^[&quot;,&quot;Issue with weak supervision in encoder-decoder setting is that decoder is not able to refine encoder representations easily, since the weak labels are not aligned with input sequence.&quot;,536871055]],[&quot;^1A&quot;,[238,&quot;^W&quot;,&quot;^1F&quot;,536871044]],[&quot;^1A&quot;,[238,&quot;^K&quot;,227,536871044]],[&quot;^1A&quot;,[238,&quot;^X&quot;,1,536871058]],[&quot;^1A&quot;,[238,&quot;^17&quot;,209,536871044]],[&quot;^1A&quot;,[238,&quot;^13&quot;,227,536871048]],[&quot;^1A&quot;,[238,&quot;^12&quot;,209,536871058]],[&quot;^1A&quot;,[238,&quot;^D&quot;,[&quot;^ &quot;],536871058]],[&quot;^1A&quot;,[238,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Issue with weak supervision in encoder-decoder setting is that decoder is not able to refine encoder representations easily, since the weak labels are not aligned with input sequence.&quot;]],536871058]],[&quot;^1A&quot;,[238,&quot;^1B&quot;,true,536871044]],[&quot;^1A&quot;,[238,&quot;^?&quot;,&quot;~u6120a666-a8c0-4cef-aa45-891a8d0fb854&quot;,536871044]],[&quot;^1A&quot;,[239,&quot;^1C&quot;,&quot;To_circumvent_this-2c-_authors_have_used_the__phase_where_they_initially_train_the_model_with_supervised_data_(all_1000_hrs_which_are_also_used_for__phase)-2e-&quot;,536871066]],[&quot;^1A&quot;,[239,&quot;^S&quot;,[],536871066]],[&quot;^1A&quot;,[239,&quot;^[&quot;,&quot;To circumvent this, authors have used the `burn-in` phase where they initially train the model with supervised data (all 1000 hrs which are also used for `fine-tune` phase).&quot;,536871066]],[&quot;^1A&quot;,[239,&quot;^W&quot;,&quot;^1F&quot;,536871059]],[&quot;^1A&quot;,[239,&quot;^K&quot;,238,536871059]],[&quot;^1A&quot;,[239,&quot;^X&quot;,1,536871066]],[&quot;^1A&quot;,[239,&quot;^17&quot;,209,536871059]],[&quot;^1A&quot;,[239,&quot;^13&quot;,227,536871059]],[&quot;^1A&quot;,[239,&quot;^12&quot;,209,536871066]],[&quot;^1A&quot;,[239,&quot;^D&quot;,[&quot;^ &quot;],536871066]],[&quot;^1A&quot;,[239,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;To circumvent this, authors have used the &quot;],[&quot;Code&quot;,&quot;burn-in&quot;],[&quot;Plain&quot;,&quot; phase where they initially train the model with supervised data (all 1000 hrs which are also used for &quot;],[&quot;Code&quot;,&quot;fine-tune&quot;],[&quot;Plain&quot;,&quot; phase).&quot;]],536871066]],[&quot;^1A&quot;,[239,&quot;^1B&quot;,true,536871059]],[&quot;^1A&quot;,[239,&quot;^?&quot;,&quot;~u6120a671-f51d-40a9-bd39-64de0dc62abc&quot;,536871059]],[&quot;^1A&quot;,[241,&quot;^1C&quot;,&quot;Unsupervised_training_in_Speech_Recognition&quot;,536871148]],[&quot;^1A&quot;,[241,&quot;^S&quot;,[],536871148]],[&quot;^1A&quot;,[241,&quot;^[&quot;,&quot;Unsupervised training in Speech Recognition&quot;,536871148]],[&quot;^1A&quot;,[241,&quot;^W&quot;,&quot;^1F&quot;,536871097]],[&quot;^1A&quot;,[241,&quot;^K&quot;,40,536871097]],[&quot;^1A&quot;,[241,&quot;^X&quot;,1,536871148]],[&quot;^1A&quot;,[241,&quot;^17&quot;,40,536871097]],[&quot;^1A&quot;,[241,&quot;^13&quot;,40,536871097]],[&quot;^1A&quot;,[241,&quot;^12&quot;,40,536871148]],[&quot;^1A&quot;,[241,&quot;^D&quot;,[&quot;^ &quot;],536871148]],[&quot;^1A&quot;,[241,&quot;^Z&quot;,[[&quot;Plain&quot;,&quot;Unsupervised training in Speech Recognition&quot;]],536871148]],[&quot;^1A&quot;,[241,&quot;^1B&quot;,true,536871097]],[&quot;^1A&quot;,[241,&quot;^?&quot;,&quot;~u6120a6f4-bb93-4574-8a4c-baa7c4a47fbe&quot;,536871097]],[&quot;^1A&quot;,[242,&quot;^1C&quot;,&quot;&quot;,536871112]],[&quot;^1A&quot;,[242,&quot;^S&quot;,[],536871125]],[&quot;^1A&quot;,[242,&quot;^[&quot;,&quot;[[Training ASR models by Generation of Contextual Information]]&quot;,536871122]],[&quot;^1A&quot;,[242,&quot;^W&quot;,&quot;^1F&quot;,536871112]],[&quot;^1A&quot;,[242,&quot;^K&quot;,241,536871112]],[&quot;^1A&quot;,[242,&quot;^X&quot;,1,536871125]],[&quot;^1A&quot;,[242,&quot;^17&quot;,40,536871112]],[&quot;^1A&quot;,[242,&quot;^13&quot;,241,536871116]],[&quot;^1A&quot;,[242,&quot;^12&quot;,40,536871125]],[&quot;^1A&quot;,[242,&quot;^12&quot;,209,536871125]],[&quot;^1A&quot;,[242,&quot;^D&quot;,[&quot;^ &quot;],536871125]],[&quot;^1A&quot;,[242,&quot;^L&quot;,209,536871125]],[&quot;^1A&quot;,[242,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;^1K&quot;,[&quot;Page_ref&quot;,&quot;Training ASR models by Generation of Contextual Information&quot;],&quot;^1N&quot;,[[&quot;Plain&quot;,&quot;&quot;]],&quot;^1O&quot;,&quot;[[Training ASR models by Generation of Contextual Information]]&quot;,&quot;^1P&quot;,&quot;&quot;]]],536871125]],[&quot;^1A&quot;,[242,&quot;^1B&quot;,true,536871112]],[&quot;^1A&quot;,[242,&quot;^?&quot;,&quot;~u6120a703-08c3-474d-b5ee-17b80483a31b&quot;,536871112]],[&quot;^1A&quot;,[243,&quot;^1C&quot;,&quot;_&quot;,536871159]],[&quot;^1A&quot;,[243,&quot;^S&quot;,[],536871162]],[&quot;^1A&quot;,[243,&quot;^[&quot;,&quot;[[Feature Replacement and Combination for Hybrid ASR Systems]] - #todo&quot;,536871162]],[&quot;^1A&quot;,[243,&quot;^W&quot;,&quot;^1F&quot;,536871126]],[&quot;^1A&quot;,[243,&quot;^K&quot;,242,536871126]],[&quot;^1A&quot;,[243,&quot;^X&quot;,1,536871162]],[&quot;^1A&quot;,[243,&quot;^17&quot;,40,536871126]],[&quot;^1A&quot;,[243,&quot;^13&quot;,241,536871126]],[&quot;^1A&quot;,[243,&quot;^12&quot;,3,536871162]],[&quot;^1A&quot;,[243,&quot;^12&quot;,40,536871162]],[&quot;^1A&quot;,[243,&quot;^12&quot;,115,536871162]],[&quot;^1A&quot;,[243,&quot;^D&quot;,[&quot;^ &quot;],536871162]],[&quot;^1A&quot;,[243,&quot;^L&quot;,3,536871162]],[&quot;^1A&quot;,[243,&quot;^L&quot;,115,536871162]],[&quot;^1A&quot;,[243,&quot;^Z&quot;,[[&quot;Link&quot;,[&quot;^ &quot;,&quot;^1K&quot;,[&quot;Page_ref&quot;,&quot;Feature Replacement and Combination for Hybrid ASR Systems&quot;],&quot;^1N&quot;,[[&quot;Plain&quot;,&quot;&quot;]],&quot;^1O&quot;,&quot;[[Feature Replacement and Combination for Hybrid ASR Systems]]&quot;,&quot;^1P&quot;,&quot;&quot;]],[&quot;Plain&quot;,&quot; - &quot;],[&quot;Tag&quot;,[[&quot;Plain&quot;,&quot;todo&quot;]]]],536871162]],[&quot;^1A&quot;,[243,&quot;^1B&quot;,true,536871126]],[&quot;^1A&quot;,[243,&quot;^?&quot;,&quot;~u6120a709-cd11-4465-a5e5-e56167c12467&quot;,536871126]],[&quot;^1A&quot;,[244,&quot;^1C&quot;,&quot;&quot;,536871165]],[&quot;^1A&quot;,[244,&quot;^S&quot;,[],536871165]],[&quot;^1A&quot;,[244,&quot;^[&quot;,&quot;&quot;,536871165]],[&quot;^1A&quot;,[244,&quot;^W&quot;,&quot;^1F&quot;,536871165]],[&quot;^1A&quot;,[244,&quot;^E&quot;,false,536871165]],[&quot;^1A&quot;,[244,&quot;^K&quot;,3,536871165]],[&quot;^1A&quot;,[244,&quot;^X&quot;,1,536871165]],[&quot;^1A&quot;,[244,&quot;^17&quot;,3,536871165]],[&quot;^1A&quot;,[244,&quot;^13&quot;,3,536871165]],[&quot;^1A&quot;,[244,&quot;^D&quot;,[&quot;^ &quot;],536871165]],[&quot;^1A&quot;,[244,&quot;^Z&quot;,[],536871165]],[&quot;^1A&quot;,[244,&quot;^1B&quot;,true,536871165]],[&quot;^1A&quot;,[244,&quot;^?&quot;,&quot;~u6120a762-d389-47ed-bfd9-98aa2b775403&quot;,536871165]]]]]]"</script><script>window.logseq_state="{:ui/theme \"dark\", :ui/cycle-collapse :show-all, :ui/sidebar-collapsed-blocks {}, :ui/show-recent? false, :config {\"local\" {:shortcuts {}, :default-templates {:journals \"\"}, :feature/enable-journals? false, :macros {}, :ui/show-empty-bullets? false, :markdown/version 2, :preferred-workflow :todo, :publishing/all-pages-public? true, :ref/default-open-blocks-level 2, :feature/enable-block-timestamps? false, :commands [], :hidden [], :default-queries {:journals [{:title \"🔨 NOW\", :query [:find (pull ?h [*]) :in $ ?start ?today :where [?h :block/marker ?marker] [(contains? #{\"NOW\" \"DOING\"} ?marker)] [?h :block/page ?p] [?p :block/journal? true] [?p :block/journal-day ?d] [(>= ?d ?start)] [(<= ?d ?today)]], :inputs [:14d :today], :result-transform (fn [result] (sort-by (fn [h] (get h :block/priority \"Z\")) result)), :collapsed? false} {:title \"📅 NEXT\", :query [:find (pull ?h [*]) :in $ ?start ?next :where [?h :block/marker ?marker] [(contains? #{\"NOW\" \"LATER\" \"TODO\"} ?marker)] [?h :block/ref-pages ?p] [?p :block/journal? true] [?p :block/journal-day ?d] [(> ?d ?start)] [(< ?d ?next)]], :inputs [:today :7d-after], :collapsed? false}]}, :editor/logical-outdenting? true, :zotero/settings {:type-id \"7048753\", :attachments-block-text \"[[Attachments]]\", :notes-block-text \"[[Notes]]\", :zotero-data-directory \"\", :include-attachments? false, :include-notes? true, :prefer-citekey? false, :page-insert-prefix \"\"}, :ui/enable-tooltip? true, :graph/settings {:orphan-pages? false, :builtin-pages? false}, :zotero/settings-v2 {\"default\" {:include-attachments? false, :include-notes? true, :prefer-citekey? false, :type-id \"7048753\", :page-insert-prefix \"\", :zotero-linked-attachment-base-directory \"/Users/utkarshchauhan/Google Drive/ResearchPapers\", :notes-block-text \"\"}}, :ui/show-command-doc? true, :default-home {:page \"Contents\"}}}}"</script><script type="text/javascript">// Single Page Apps for GitHub Pages
      // https://github.com/rafgraph/spa-github-pages
      // Copyright (c) 2016 Rafael Pedicini, licensed under the MIT License
      // ----------------------------------------------------------------------
      // This script checks to see if a redirect is present in the query string
      // and converts it back into the correct url and adds it to the
      // browser's history using window.history.replaceState(...),
      // which won't cause the browser to attempt to load the new url.
      // When the single page app is loaded further down in this file,
      // the correct url will be waiting in the browser's history for
      // the single page app to route accordingly.
      (function(l) {
        if (l.search) {
          var q = {};
          l.search.slice(1).split('&').forEach(function(v) {
            var a = v.split('=');
            q[a[0]] = a.slice(1).join('=').replace(/~and~/g, '&');
          });
          if (q.p !== undefined) {
            window.history.replaceState(null, null,
              l.pathname.slice(0, -1) + (q.p || '') +
              (q.q ? ('?' + q.q) : '') +
              l.hash
            );
          }
        }
      }(window.location))</script><script src="static/js/highlight.min.js"></script><script src="static/js/interact.min.js"></script><script src="static/js/main.js"></script></body>